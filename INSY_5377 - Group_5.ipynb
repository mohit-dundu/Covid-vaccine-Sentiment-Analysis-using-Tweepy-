{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENT & EXPLORATORY DATA ANALYSIS OF TWEETS ON COVID-19 VACCINES\n",
    "\n",
    "## INSY 5377- 001 : Web And Social Analytics\n",
    "\n",
    "## Group-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code has been inspired from the following sources:\n",
    "- https://www.kaggle.com/datasets/gpreda/all-covid19-vaccines-tweets/code?resource=download (Dataset)\n",
    "- https://www.kaggle.com/code/hassanhshah/covid-vaccine-sentiment-and-time-series-analysis (Time Series Analysis VADER)\n",
    "- https://www.kaggle.com/code/yutotsubaki/sentiment-analysis-with-textblob-analyze-in-time (Textblob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure',figsize=(17,13))\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from wordcloud import WordCloud,STOPWORDS, ImageColorGenerator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Library Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data\n",
    "vaccine_filepath = r'C:\\Users\\dundu\\OneDrive\\Desktop\\Web_Social_Project\\MAIN_MOHIT.csv'\n",
    "vaccine_data = pd.read_csv(vaccine_filepath)\n",
    "print(\"Read Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(vaccine_data.shape, str(vaccine_data.shape[0])+\" tweets in dataset\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine_data=vaccine_data.drop_duplicates('text') #dropping duplicate tweets\n",
    "vaccine_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining Data\n",
    "vaccine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining Data\n",
    "vaccine_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining statistics\n",
    "vaccine_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining data types\n",
    "vaccine_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for unfilled values\n",
    "vaccine_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "vaccine_data['text'] = vaccine_data['text'].str.lower()\n",
    "vaccine_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Removal\n",
    "vaccine_data['text'] = vaccine_data['text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "vaccine_data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation Removal\n",
    "punctuation_removal = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', punctuation_removal))\n",
    "vaccine_data[\"text\"] = vaccine_data[\"text\"].apply(lambda text: remove_punctuation(text))\n",
    "vaccine_data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single character and double space removal\n",
    "vaccine_data[\"text\"] = vaccine_data[\"text\"].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
    "vaccine_data[\"text\"] = vaccine_data[\"text\"].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "vaccine_data[\"text\"]\n",
    "vaccine_data.to_csv(\"1.Before_clean.csv\")\n",
    "vaccine_filepath1 = r'C:\\Users\\dundu\\OneDrive\\Desktop\\Web_Social_Project\\1.Before_clean.csv'\n",
    "vaccine_data1 = pd.read_csv(vaccine_filepath1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword Removal\n",
    "\", \".join(stopwords.words('english'))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "vaccine_data[\"text\"] = vaccine_data[\"text\"].apply(lambda text: remove_stopwords(text))\n",
    "vaccine_data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji Removal\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "vaccine_data[\"text\"] = vaccine_data[\"text\"].apply(str)\n",
    "vaccine_data[\"text\"] = vaccine_data[\"text\"].apply(remove_emoji)\n",
    "vaccine_data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single character and double space removal\n",
    "vaccine_data[\"text\"] = vaccine_data[\"text\"].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
    "vaccine_data[\"text\"] = vaccine_data[\"text\"].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "vaccine_data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in vaccine_data[\"text\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "sid = SIA()\n",
    "vaccine_data['sentiments'] = vaccine_data[\"text\"].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',str(x).lower()))))\n",
    "vaccine_data['Positive Sentiment'] = vaccine_data['sentiments'].apply(lambda x: x['pos']) \n",
    "vaccine_data['Neutral Sentiment'] = vaccine_data['sentiments'].apply(lambda x: x['neu'])\n",
    "vaccine_data['Negative Sentiment'] = vaccine_data['sentiments'].apply(lambda x: x['neg'])\n",
    "\n",
    "vaccine_data['Compound'] = vaccine_data['sentiments'].apply(lambda x: x['compound'])\n",
    "sentiment=[]\n",
    "scores = vaccine_data['Compound'].tolist()\n",
    "for i in scores:\n",
    "    if i>=0.05:\n",
    "        sentiment.append('Positive')\n",
    "    elif i<=(-0.05):\n",
    "        sentiment.append('Negative')\n",
    "    else:\n",
    "        sentiment.append('Neutral')\n",
    "vaccine_data['sentiment']=pd.Series(np.array(sentiment))\n",
    "\n",
    "\n",
    "\n",
    "vaccine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine_data1 = vaccine_data\n",
    "\n",
    "# def sentiment_analysis(tweet):\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "#Function for Subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "#Function for polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "vaccine_data1['TextBlob_Subjectivity'] = vaccine_data1['text'].apply(getSubjectivity)\n",
    "vaccine_data1['TextBlob_Polarity'] = vaccine_data1['text'].apply(getPolarity)\n",
    "def getAnalysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "vaccine_data1['TextBlob_Analysis'] = vaccine_data1['TextBlob_Polarity'].apply(getAnalysis)\n",
    "vaccine_data1.to_csv(\"TEXTBLOB.CSV\")\n",
    "vaccine_data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "sid = SIA()\n",
    "vaccine_data1['sentiments'] = vaccine_data1[\"text\"].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',str(x).lower()))))\n",
    "vaccine_data1['Positive Sentiment'] = vaccine_data1['sentiments'].apply(lambda x: x['pos']) \n",
    "vaccine_data1['Neutral Sentiment'] = vaccine_data1['sentiments'].apply(lambda x: x['neu'])\n",
    "vaccine_data1['Negative Sentiment'] = vaccine_data1['sentiments'].apply(lambda x: x['neg'])\n",
    "vaccine_data1.head()\n",
    "vaccine_data1.to_csv(\"1.Unclean_test_sent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine_data.to_csv(\"1.After_Sent_Analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Words\n",
    "vaccine_data['Number_Of_Words'] = vaccine_data['text'].apply(lambda x:len(x.split(' ')))\n",
    "#Average Word Length\n",
    "vaccine_data['Mean_Word_Length'] = vaccine_data['text'].apply(lambda x:np.round(np.mean([len(w) for w in x.split(' ')]),2) )\n",
    "vaccine_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and lemmatization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "vaccine_data['tokenized'] = vaccine_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "vaccine_data['lemmatized'] = vaccine_data['tokenized'].apply(lambda x: lemmatizer(x))\n",
    "vaccine_data.head()\n",
    "vaccine_data.tail()\n",
    "vaccine_data.to_csv(\"1.Token_Lemm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Cloud\n",
    "tweet_All = \" \".join(review for review in vaccine_data[\"text\"])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize  = (10,10))\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud_ALL = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_All)\n",
    "\n",
    "# Display the generated image:\n",
    "ax.imshow(wordcloud_ALL, interpolation='bilinear')\n",
    "\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Distribution\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Distriubtion Of Sentiments Across Tweets',fontsize=19,fontweight='bold')\n",
    "sns.kdeplot(vaccine_data['Negative Sentiment'], color = 'red')\n",
    "sns.kdeplot(vaccine_data['Positive Sentiment'], color = 'green')\n",
    "sns.kdeplot(vaccine_data['Neutral Sentiment'], color = 'blue')\n",
    "plt.xlabel(' ')\n",
    "plt.legend(['Negative Sentiment','Positive Sentiment','Neutral Sentiment'])\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Average Sentiments Across Tweets',fontsize=19,fontweight='bold')\n",
    "neg_total_avg = (vaccine_data['Negative Sentiment'].sum())/len(vaccine_data.index)\n",
    "print(neg_total_avg)\n",
    "pos_total_avg = (vaccine_data['Positive Sentiment'].sum())/len(vaccine_data.index)\n",
    "print(pos_total_avg)\n",
    "neu_total_avg = (vaccine_data['Neutral Sentiment'].sum())/len(vaccine_data.index)\n",
    "print(neu_total_avg)\n",
    "sentiment_type = ['Negative','Positive','Neutral']\n",
    "sentiment_total_avg = [neg_total_avg, pos_total_avg, neu_total_avg]\n",
    "plt.bar(sentiment_type, sentiment_total_avg, color = ['red', 'green', 'blue'])\n",
    "plt.ylabel('Average Sentiment Per Tweet',fontsize=19)\n",
    "plt.xlabel('Sentiment Type',fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Destribution\n",
    "neg_total_avg = (vaccine_data['Negative Sentiment'].sum())/len(vaccine_data.index)\n",
    "print(\"Average Density of Negative Tweets:\", neg_total_avg)\n",
    "pos_total_avg = (vaccine_data['Positive Sentiment'].sum())/len(vaccine_data.index)\n",
    "print(\"Average Density of Positive Tweets: \",pos_total_avg)\n",
    "neu_total_avg = (vaccine_data['Neutral Sentiment'].sum())/len(vaccine_data.index)\n",
    "print(\"Average Density of Neutral Tweets: \",neu_total_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords are the words which won't bring about any changes to the polarity of the tweet\n",
    "stop_words = stopwords.words('english')   \n",
    "len(stop_words),stop_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change variables\n",
    "ft_data = vaccine_data.copy()\n",
    "ft_data['date'] = pd.to_datetime(vaccine_data['date']).dt.date\n",
    "ft_data['year'] = pd.DatetimeIndex(ft_data['date']).year\n",
    "b_date_count = ft_data.groupby(by='date').count().reset_index()\n",
    "b_date_count = b_date_count.rename(columns={'id':'Tweets Per Day'})\n",
    "fig = ex.line(b_date_count,x='date',y='Tweets Per Day')\n",
    "\n",
    "fig.add_shape(type=\"line\",\n",
    "    x0=b_date_count['date'].values[0], y0=b_date_count['Negative Sentiment'].mean(), x1=b_date_count['date'].values[-1], y1=b_date_count['Negative Sentiment'].mean(),\n",
    "    line=dict(\n",
    "        color=\"Red\",\n",
    "        width=2,\n",
    "        dash=\"dashdot\",\n",
    "    ),\n",
    "        name='Mean',\n",
    ")\n",
    "\n",
    "fig.update_traces(mode=\"markers+lines\")\n",
    "fig.update_layout(hovermode=\"x unified\")\n",
    "\n",
    "\n",
    "# ###annots\n",
    "b_date_count.date = pd.to_datetime(b_date_count.date)\n",
    "b_date_count_dt = b_date_count.set_index('date')\n",
    "\n",
    "fig.add_annotation(x=datetime.datetime(2021,3,1), y=b_date_count_dt.loc[pd.Timestamp('2021-03-1'),'year'],\n",
    "            text=r\"J&J authorization\",\n",
    "            showarrow=True,\n",
    "            arrowhead= 3,\n",
    "            bordercolor=\"#c7c7c7\")\n",
    "\n",
    "fig.add_annotation(x=datetime.datetime(2021,4,21), y=b_date_count_dt.loc[pd.Timestamp('2021-04-21'),'year'],\n",
    "            text=r\"Fear of supply outstripping demand & CDC discussion of J&J bloodclots\",\n",
    "            showarrow=True,\n",
    "            arrowhead=3,\n",
    "            yshift=5,bordercolor=\"#c7c7c7\")\n",
    "\n",
    "fig.add_annotation(x=datetime.datetime(2021,6,29), y=b_date_count_dt.loc[pd.Timestamp('2021-06-29'),'year'],\n",
    "            text=r\"Discussion of vaccine protection against delta variant\",\n",
    "            showarrow=True,\n",
    "            arrowhead=3,\n",
    "            yshift=5,ay=-30,bordercolor=\"#c7c7c7\")\n",
    "\n",
    "fig.update_layout(title='<b>Daily Tweets<b>',width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Negative Neutral Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning sentiment \n",
    "Positive_tweet = vaccine_data[vaccine_data['Positive Sentiment'] >= 0.5].reset_index()\n",
    "Negative_tweet = vaccine_data[vaccine_data['Negative Sentiment']>= 0.5].reset_index()\n",
    "Neutral_tweet = vaccine_data[vaccine_data['Neutral Sentiment']>= 0.5].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 most common positive words\n",
    "\n",
    "all_positive_words=[]\n",
    "for i in range(len(Positive_tweet['lemmatized'])):\n",
    "    a=Positive_tweet['lemmatized'][i]\n",
    "    for i in a:\n",
    "        all_positive_words.append(i)\n",
    "all_positive_words=pd.Series(np.array(all_positive_words))\n",
    "common_words=all_positive_words.value_counts()[:50].rename_axis('Common Positive Words').reset_index(name='count')\n",
    "fig = ex.treemap(common_words, path=['Common Positive Words'], values='count',title='50 Most Common Words In Positive Tweets')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 most common negative words\n",
    "\n",
    "all_negative_words=[]\n",
    "for i in range(len(Negative_tweet['lemmatized'])):\n",
    "    a=Negative_tweet['lemmatized'][i]\n",
    "    for i in a:\n",
    "        all_negative_words.append(i)\n",
    "all_negative_words=pd.Series(np.array(all_negative_words))\n",
    "common_words=all_negative_words.value_counts()[:50].rename_axis('Common Negative Words').reset_index(name='count')\n",
    "fig = ex.treemap(common_words, path=['Common Negative Words'], values='count',title='50 Most Common Words In Negative Tweets')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 most common neutral words\n",
    "\n",
    "all_neutral_words=[]\n",
    "for i in range(len(Neutral_tweet['lemmatized'])):\n",
    "    a=Neutral_tweet['lemmatized'][i]\n",
    "    for i in a:\n",
    "        all_neutral_words.append(i)\n",
    "all_neutral_words=pd.Series(np.array(all_neutral_words))\n",
    "common_words=all_neutral_words.value_counts()[:50].rename_axis('Common Neutral Words').reset_index(name='count')\n",
    "fig = ex.treemap(common_words, path=['Common Neutral Words'], values='count',title='50 Most Common Words In Neutral Tweets')\n",
    "fig.show()\n",
    "vaccine_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vax_data = r'C:\\Users\\dundu\\OneDrive\\Desktop\\Web_Social_Project\\Main_Data\\vaccination_all_tweets.csv'\n",
    "df_vax = pd.read_csv(vax_data)\n",
    "print(\"Read Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vax= df_vax.drop(['user_name','user_description','user_created','user_followers','user_friends','user_favourites','source','is_retweet'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vax = ['covaxin', 'sinopharm', 'sinovac', 'moderna', 'pfizer', 'biontech', 'oxford', 'astrazeneca', 'sputnik']\n",
    "vaccine_filepath = r'C:\\Users\\dundu\\OneDrive\\Desktop\\Web_Social_Project\\Main_Data\\vaccination_all_tweets.csv'\n",
    "vax_sentiment = pd.DataFrame()\n",
    "vax_sentiment['Vaccine']=all_vax\n",
    "sentiments=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vax= vaccine_data.drop(['user_name','user_description','user_created','user_followers','user_friends','user_favourites','source','is_retweet'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_sentiment=pd.DataFrame()\n",
    "countries=['india','usa','canada','germany','spain','pakistan','uk','brazil','russia','italy','australia','france','argentina','uae','israel','mexico','japan']\n",
    "country_sentiment['countries']=countries\n",
    "senti=list()\n",
    "\n",
    "for country in countries :\n",
    "    senti.append(vax[vax['user_location'].str.lower().str.contains(country)].Sentiment.mean())\n",
    "    \n",
    "country_sentiment['Sentiment']=senti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine_data['word_sentiment']=pd.Series(np.array(sentiment))\n",
    "vaccine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(10, 16))\n",
    "sns.barplot(x=\"user_followers\", y=\"user_name\", orient=\"h\", ax=ax1, palette=[\"b\"],\n",
    "           vaccine_data=vaccine_data[(vaccine_data.sentiment== \"Positive\")]\\\n",
    "           .drop_duplicates(subset=[\"user_name\"])\\\n",
    "           .sort_values(by=[\"user_followers\"], ascending=False)[[\"user_name\", \"user_followers\"]][:10])\n",
    "ax1.set_title('Top 10 Accounts with Highest Followers who tweet Positive')\n",
    "sns.barplot(x=\"user_followers\", y=\"user_name\", orient=\"h\", ax=ax2, palette=[\"g\"],\n",
    "           vaccine_data=vaccine_data[(vaccine_data.sentiment == \"Neutral\")]\n",
    "           .drop_duplicates(subset=[\"user_name\"])\\\n",
    "           .sort_values(by=[\"user_followers\"], ascending=False)[[\"user_name\", \"user_followers\"]][:10])\n",
    "ax2.set_title('Top 10 Accounts with Highest Followers who tweet Neutral')\n",
    "sns.barplot(x=\"user_followers\", y=\"user_name\", orient=\"h\", ax=ax3, palette=[\"r\"],\n",
    "           vaccine_data=vaccine_data[(vaccine_data.sentiment == \"Negative\")]\n",
    "           .drop_duplicates(subset=[\"user_name\"])\\\n",
    "           .sort_values(by=[\"user_followers\"], ascending=False)[[\"user_name\", \"user_followers\"]][:10])\n",
    "ax3.set_title('Top 10 Accounts with Highest Followers who tweet Negative')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
